{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c67e806c-e0e6-415b-8cf0-ed92eba5ed37",
    "_uuid": "34b6997bb115a11f47a7f54ce9d9052791b2e707"
   },
   "source": [
    "# Overview\n",
    "Since Silicon Valley and Medium managed to make a Hot Dog / Not Hot Dog classifier, we should be able to do something similar with the free GPU Kaggle gives us. We use a pretrained VGG16 as a starting point and then add a few layers on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b073998-389e-4972-9806-ff4297767a03",
    "_uuid": "1f315ce4c0414edd872988eb5b4017a5c208eb8a",
    "collapsed": true
   },
   "source": [
    "### Copy\n",
    "copy the weights and configurations for the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e94de3e7-de1f-4c18-ad1f-c8b686127340",
    "_uuid": "c163e45042a69905855f7c04a65676e5aca4837b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.keras\n",
    "!mkdir ~/.keras/models\n",
    "!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n",
    "!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "c3cc4285-bfa4-4612-ac5f-13d10678c09a",
    "_uuid": "725d378daf5f836d4885d67240fc7955f113309d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from glob import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage.util.montage import montage2d\n",
    "from skimage.io import imread\n",
    "base_dir = os.path.join('..', 'input', 'food41')\n",
    "base_image_dir = os.path.join('..', 'input', 'food41')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "388c71d4-cd48-4bde-99bd-92e101b86f92",
    "_uuid": "5185a60363286d605796148722f046442db33794"
   },
   "source": [
    "## Preprocessing\n",
    "Find and read image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1d79959c-4921-48f9-a660-f1d550cf3b1d",
    "_uuid": "2a7d23fce33af0de54202048f00fcd23e9d876c9"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "image_paths = glob(os.path.join(base_image_dir, 'images', '*', '*'))\n",
    "print('Total Images Files', len(image_paths))\n",
    "all_paths_df = pd.DataFrame(dict(path = image_paths))\n",
    "all_paths_df['food_name'] = all_paths_df['path'].map(lambda x: x.split('/')[-2].replace('_', ' ').strip())\n",
    "all_paths_df['source'] = all_paths_df['food_name'].map(lambda x: 'Hot Dog' if x.lower() == 'hot dog' else 'Not Hot Dog')\n",
    "source_enc = LabelEncoder()\n",
    "all_paths_df['source_id'] = source_enc.fit_transform(all_paths_df['source'])\n",
    "all_paths_df['source_vec'] = all_paths_df['source_id'].map(lambda x: to_categorical(x, len(source_enc.classes_)))\n",
    "all_paths_df['file_id'] = all_paths_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "all_paths_df['file_ext'] = all_paths_df['path'].map(lambda x: os.path.splitext(x)[1][1:])\n",
    "# balance a bit\n",
    "all_paths_df = all_paths_df.groupby(['source_id']).apply(lambda x: x.sample(min(5000, x.shape[0]))\n",
    "                                                      ).reset_index(drop = True)\n",
    "all_paths_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "818da6ca-bbff-4ca0-ad57-ef3a145ae863",
    "_uuid": "688e4340238e013b8459b6f6470993c7de492d83"
   },
   "source": [
    "# Examine the distributions\n",
    "Show how the data is distributed and why we need to balance it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5c8bd288-8261-4cbe-a954-e62ac795cc3e",
    "_uuid": "60a8111c4093ca6f69d27a4499442ba7dd750839"
   },
   "outputs": [],
   "source": [
    "all_paths_df['source'].hist(figsize = (20, 7), xrot = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ba697ed-85bb-4e9a-9765-4c367db078d1",
    "_uuid": "4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d"
   },
   "source": [
    "# Split Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1192c6b3-a940-4fa0-a498-d7e0d400a796",
    "_uuid": "a48b300ca4d37a6e8b39f82e3c172739635e4baa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "raw_train_df, valid_df = train_test_split(all_paths_df, \n",
    "                                   test_size = 0.25, \n",
    "                                   random_state = 2018,\n",
    "                                   stratify = all_paths_df[['source_id']])\n",
    "print('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])\n",
    "raw_train_df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8060459-da1e-4293-8f61-c7f99de1de9f",
    "_uuid": "26e566d6cec5bd41f9afe392f456ddf7ceb306ea"
   },
   "source": [
    "# Balance the distribution in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "21b5d30f-c645-41ad-85bc-4b51d237c950",
    "_uuid": "dba4c54209e5ade39d771a7f5d9d995321f0a367"
   },
   "outputs": [],
   "source": [
    "train_df = raw_train_df.groupby(['source_id']).apply(lambda x: x.sample(3000, replace = True)\n",
    "                                                      ).reset_index(drop = True)\n",
    "print('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\n",
    "train_df['food_name'].hist(bins = 101, figsize = (20, 5), xrot = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9954bfda-29bd-4c4d-b526-0a972b3e43e2",
    "_uuid": "9529ab766763a9f122786464c24ab1ebe22c6006",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "##### INSERT_MODEL_IMPORT #######\n",
    "from PIL import Image\n",
    "ppi = lambda x: Image.fromarray(preprocess_input(np.array(x).astype(np.float32)))\n",
    "IMG_SIZE = (299, 299) # slightly smaller than vgg16 normally expects\n",
    "core_idg = ImageDataGenerator(samplewise_center=False, \n",
    "                              samplewise_std_normalization=False, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range = 0.1, \n",
    "                              width_shift_range = 0.1, \n",
    "                              rotation_range = 5, \n",
    "                              shear_range = 0.01,\n",
    "                              fill_mode = 'reflect',\n",
    "                              zoom_range=0.15, \n",
    "                             preprocessing_function = preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b5767f42-da63-4737-8f50-749c1a25aa84",
    "_uuid": "07851e798db3d89ba13db7d4b56ab2b759221464",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n",
    "    base_dir = os.path.dirname(in_df[path_col].values[0])\n",
    "    print('## Ignore next message from keras, values are replaced anyways')\n",
    "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
    "                                     class_mode = 'sparse',\n",
    "                                    **dflow_args)\n",
    "    df_gen.filenames = in_df[path_col].values\n",
    "    df_gen.classes = np.stack(in_df[y_col].values)\n",
    "    df_gen.samples = in_df.shape[0]\n",
    "    df_gen.n = in_df.shape[0]\n",
    "    df_gen._set_index_array()\n",
    "    df_gen.directory = '' # since we have the full path\n",
    "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "810bd229-fec9-43c4-b3bd-afd62e3e9552",
    "_uuid": "1848f5048a9e00668c3778a85deea97f980e4f1c"
   },
   "outputs": [],
   "source": [
    "train_gen = flow_from_dataframe(core_idg, train_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'source_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = 16)\n",
    "\n",
    "valid_gen = flow_from_dataframe(core_idg, valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'source_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = 16) # we can use much larger batches for evaluation\n",
    "# used a fixed dataset for evaluating the algorithm\n",
    "test_X, test_Y = next(flow_from_dataframe(core_idg, \n",
    "                               valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'source_vec', \n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'rgb',\n",
    "                            batch_size = 1024)) # one big batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2d62234f-aeb0-4eba-8a38-d713d819abf6",
    "_uuid": "8190b4ad60d49fa65af074dd138a19cb8787e983"
   },
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n",
    "for (tc_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_x = np.clip((tc_x-tc_x.min())/(tc_x.max()-tc_x.min())*255, 0 , 255).astype(np.uint8)[:,:,::]\n",
    "    c_ax.imshow(c_x[:,:])\n",
    "    c_ax.set_title('%s' % source_enc.classes_[np.argmax(c_y)])\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da22790a-672c-474e-b118-9eef15b53160",
    "_uuid": "55d665e1e8a8d83b9db005a66a965f8a90c62da1"
   },
   "source": [
    "# Pretrained Features\n",
    "Here we generate the pretrained features for a large batch of images to accelerate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "a4728bc5-c940-4dea-9e1f-e9fbcb1dab3a",
    "_uuid": "1587324de022f5342aee55445dc92731e9fb9b85"
   },
   "outputs": [],
   "source": [
    "# clean up resources\n",
    "import gc\n",
    "gc.enable()\n",
    "print(gc.collect())\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "731c14eb-5db3-4f89-b433-d372cc95ec36",
    "_uuid": "2aef067b87e53e1aa9490d86cdf10bcbc2398303"
   },
   "source": [
    "# Attention Model\n",
    "The basic idea is that a Global Average Pooling is too simplistic since some of the regions are more relevant than others. So we build an attention mechanism to turn pixels in the GAP on an off before the pooling and then rescale (Lambda layer) the results based on the number of pixels. The model could be seen as a sort of 'global weighted average' pooling. There is probably something published about it and it is very similar to the kind of attention models used in NLP.\n",
    "It is largely based on the insight that the winning solution annotated and trained a UNET model to segmenting the hand and transforming it. This seems very tedious if we could just learn attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "eeb36110-0cde-4450-a43c-b8f707adb235",
    "_uuid": "1f0dfaccda346d7bc4758e7329d61028d254a8d6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, AvgPool2D, ActivityRegularization\n",
    "from keras.models import Model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \n",
    "                              include_top = False, weights = 'imagenet')\n",
    "real_input = Input(shape=t_x.shape[1:])\n",
    "pt_features = base_pretrained_model(real_input)\n",
    "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "from keras.layers import BatchNormalization\n",
    "bn_features = BatchNormalization()(pt_features)\n",
    "# here we do an attention mechanism to turn pixels in the GAP on an off\n",
    "attn_layer = Conv2D(128, kernel_size = (1,1), padding = 'same', activation = 'elu')(bn_features)\n",
    "attn_layer = Conv2D(32, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\n",
    "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\n",
    "attn_layer = AvgPool2D((2,2), strides = (1,1), padding = 'same')(attn_layer) # smooth results\n",
    "attn_layer = Conv2D(1, \n",
    "                    kernel_size = (1,1), \n",
    "                    padding = 'valid', \n",
    "                    activation = 'sigmoid')(attn_layer)\n",
    "# fan it out to all of the channels\n",
    "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "up_c2.trainable = False\n",
    "attn_layer = up_c2(attn_layer)\n",
    "\n",
    "mask_features = multiply([attn_layer, bn_features])\n",
    "# we dont need the attention layer now\n",
    "use_attention = False\n",
    "if use_attention:\n",
    "    gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "    gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "    # to account for missing values from the attention model\n",
    "    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "else:\n",
    "    gap = GlobalAveragePooling2D()(bn_features)\n",
    "gap_dr = Dropout(0.5)(gap)\n",
    "dr_steps = Dropout(0.5)(Dense(128, activation = 'elu')(gap_dr))\n",
    "out_layer = Dense(len(source_enc.classes_), \n",
    "                  activation = 'softmax')(dr_steps)\n",
    "\n",
    "attn_model = Model(inputs = [real_input], \n",
    "                   outputs = [out_layer], name = 'attention_model')\n",
    "\n",
    "attn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                           metrics = ['categorical_accuracy'])\n",
    "\n",
    "attn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "17803ae1-bed8-41a4-9a2c-e66287a24830",
    "_uuid": "48b9764e16fb5af52aed35c82bae6299e67d5bc7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('tb_attn')\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                   factor=0.8, \n",
    "                                   patience=5,\n",
    "                                   verbose=1, \n",
    "                                   mode='auto', \n",
    "                                   epsilon=0.0001, \n",
    "                                   cooldown=5, min_lr=1e-5)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58a75586-442b-4804-84a6-d63d5a42ea14",
    "_uuid": "b2148479bfe41c5d9fd0faece4c75adea509dabe"
   },
   "outputs": [],
   "source": [
    "train_gen.batch_size = 32\n",
    "attn_model.fit_generator(train_gen, \n",
    "              validation_data = (test_X, test_Y), \n",
    "               shuffle = True,\n",
    "              epochs = 10, \n",
    "              callbacks = callbacks_list,\n",
    "                workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d0c45b0-bb23-48d2-83eb-bc3990043e26",
    "_uuid": "3a90f05dd206cd76c72d8c6278ebb93da41ee45f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the best version of the model\n",
    "attn_model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f038f41-0560-4e9c-a49a-4cba33ff6575",
    "_uuid": "3f0e8d1d656b4beb3f2208c2bdaf69ecd41cd29c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.keras # clean up the model / make space for other things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24796de7-b1e9-4b3b-bcc6-d997aa3e6d16",
    "_uuid": "244bac80d1ea2074e47932e367996e32cbab6a3d"
   },
   "source": [
    "# Evaluate the results\n",
    "Here we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "d0edaf00-4b7c-4f65-af0b-e5a03b9b8428",
    "_uuid": "b421b6183b1919a7414482f0b1ac611079e45174"
   },
   "outputs": [],
   "source": [
    "pred_Y = attn_model.predict(test_X, \n",
    "                          batch_size = 16, \n",
    "                          verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "15189df2-3fed-495e-9661-97bb2b712dfd",
    "_uuid": "10162e055ca7cd52878a289bab377231787ab732"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize, dpi = 300)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig\n",
    "\n",
    "print_confusion_matrix(confusion_matrix(np.argmax(test_Y,-1), \n",
    "                             np.argmax(pred_Y,-1), labels = range(test_Y.shape[1])), \n",
    "                       class_names = source_enc.classes_, figsize = (10, 1)).savefig('confusion_matrix.png')\n",
    "\n",
    "print(classification_report(np.argmax(test_Y,-1), \n",
    "                            np.argmax(pred_Y,-1), \n",
    "                            target_names = source_enc.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "0fd5f7335bf5f5ceb2177e852f038337c92aa21d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "tpr, fpr, _ = roc_curve(np.argmax(test_Y,-1), pred_Y[:, 1])\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "ax1.plot(tpr, fpr, 'r.', label = 'ROC (%2.2f)' % (roc_auc_score(np.argmax(test_Y,-1), pred_Y[:, 1])))\n",
    "ax1.plot(tpr, tpr, 'b-')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "91a6cac6-ced4-4f1d-a52b-2ea998c86eec",
    "_uuid": "583fc085ff387b4227c3a45208dfa0a019521f3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model.save('full_pred_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43aebf67-c55b-4a2a-968e-7ac6dcdb54d1",
    "_uuid": "d3f5425b358f9d8358e5347e487f4c7352287b09",
    "collapsed": true
   },
   "source": [
    "# Examine Misclassifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "1d471eae51200b984ecfd8d8aaa1e4ddc9f11a88"
   },
   "outputs": [],
   "source": [
    "class_df = pd.DataFrame(dict(label = np.argmax(test_Y,-1), \n",
    "                             prediction= pred_Y[:, 1]))\n",
    "class_df['mismatch'] = np.abs(class_df['label']-class_df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "b971504c3d812d981c17f9de4af41437c7cacb89"
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(2, 5, figsize = (20, 8))\n",
    "for (c_grp, c_df), c_axs in zip(class_df.groupby('label'), m_axs):\n",
    "    for (c_idx, c_row), c_ax in zip(c_df.sort_values('mismatch', ascending = False).iterrows(), c_axs):\n",
    "        tc_x = test_X[c_idx]\n",
    "        c_x = np.clip((tc_x-tc_x.min())/(tc_x.max()-tc_x.min())*255, 0 , 255).astype(np.uint8)[:,:,::]\n",
    "        c_ax.imshow(c_x[:,:])\n",
    "        c_ax.set_title('Actual: %s\\n(HotDog Pred: %2.2f%%)' % (source_enc.classes_[int(c_row['label'])], 100*(1-c_row['prediction'])))\n",
    "        c_ax.axis('off')\n",
    "fig.savefig('mismatch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b06350db3b728eb5e9c054a92a934fa3be4b40c7",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
